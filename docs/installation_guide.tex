\title{OpenSBLI setup guide}
\author{David J. Lusher: d.lusher@soton.ac.uk}
\documentclass[11pt]{article}

\usepackage{nth}
\usepackage{hyperref}

\begin{document}
\maketitle

OpenSBLI requires a Python 2.7 install for code generation, and the Oxford Parallel Structured software (OPS) library to build executables. The guide is for Linux platforms only, and hasn't been tested on OSX/Windows. Python packages are installed with pip, if pip is not already present on your system it can be installed with e.g. \verb|sudo apt-get install python-pip|, on Ubuntu/Debian based systems. 

The guide is for installation on a local machine with the GNU C compiler (gcc) with openMPI, on HPC clusters you will have to modify the steps/load the necessary modules to the available compilers, MPI distribution and HDF5 libraries. It is important that the HDF5 installation was built with the same compiler/MPI distrubution that you are using to build the code. For generating CUDA, OpenCL, or OpenACC code, these libraries should be installed and their location exported in step 4. OPS and OpenSBLI are installed to your home directory, if you wish to have them elsewhere then modify the locations accordingly.


\begin{enumerate}
\item{Install OpenMPI and HDF5 libraries, git and pip/tk (skip this step if using a machine with pre-configured software modules):
\begin{verbatim}
sudo apt-get install python-pip python-tk
sudo apt-get install libhdf5-openmpi-dev
sudo apt-get install git
\end{verbatim}}
\item{Install the required Python packages for this user:}
\begin{verbatim}
pip install --user sympy scipy numpy h5py matplotlib flake8
\end{verbatim}
\item{Clone the OPS library:\\ \verb|git clone https://github.com/OP-DSL/OPS.git|}
\item{Export OPS environment variables:\\ Edit your \verb|~/.bashrc| to include the lines:}
\begin{verbatim}
export OPS_INSTALL_PATH=/home/<username>/OPS/ops/
export OPS_COMPILER=gnu
export OPS_TRANSLATOR=/home/<username>/OPS/ops_translator/c/
export MPI_INSTALL_PATH=/usr/
export HDF5_INSTALL_PATH=/usr/lib/x86_64-linux-gnu/hdf5/openmpi/
export CUDA_INSTALL_PATH=/path/to/cuda-install/
\end{verbatim}
To make these changes active in the current shell type: \verb|source ~/.bashrc|
\item{Build the OPS libaries:}
\begin{verbatim}
Change directory to ~/OPS/ops/c/
make core
make seq
make mpi
\end{verbatim}
If you are using CUDA you will also have to build:
\begin{verbatim}
make cuda
make mpi_cuda
\end{verbatim}

\item{Clone the latest OpenSBLI with your username and change branch:}
\begin{verbatim}
git clone https://<your-username>@bitbucket.org/spjammy/opensbliweno.git
cd opensbliweno
git fetch && git checkout feature/curvilinear
\end{verbatim}

\item{Export OpenSBLI to your PYTHONPATH in \verb|~/.bashrc|:}
\begin{verbatim}
export PYTHONPATH=$PYTHONPATH:/home/<username>/opensbliweno/
\end{verbatim}

\item{Generate a code in OpenSBLI:}
\begin{verbatim}
cd ~/opensbliweno/apps/restructured/TGV/
python taylor_green_vortex.py
\end{verbatim}
You should now have four new files:\\ \verb|opensbli.cpp, opensbliblock00_kernels.h, defdec_data_set.h, bc_exchanges.h| and some LaTeX files.
\item{Translate the code using OPS to generate parallel versions:}
\begin{verbatim}
python $OPS_TRANSLATOR/ops.py opensbli.cpp
\end{verbatim}
You should now have folders for CUDA, MPI and so on, plus an \verb|opensbli_ops.cpp| file. You need to apply the translation step each time you make changes to the opensbli.cpp code generated by OpenSBLI. 

\item{Copy a Makefile into the directory and build the executable for the desired architecture (MPI, CUDA, OpenMP etc):}
\begin{verbatim}
cp ../Makefile ./
make opensbli_mpi
\end{verbatim}

\item{Run the executable for number of processes 'np':}
\begin{verbatim}
mpirun -np 4 ./opensbli_mpi
\end{verbatim}
The simulation will now run, when finished the output data is written to an \verb|opensbli_output.h5| HDF5 file for post-processing. If you wish to run the simulation again remove the HDF5 file from the directory before repeating the above steps.

\end{enumerate}

\textbf{Things to note for HPC clusters:}\\
\begin{enumerate}
\item{The main difference when using HPC clusters is the locations specified in step 4 will be different. Depending on your machine you will have to load modules for a compiler, MPI distribution, CUDA and HDF5. You will need to find out where on the machine these libaries are located and export them as required.}

\item{For different compilers the \verb|OPS_COMPILER| variable must be changed, and you may need to modify some of the Makefiles. The easiest way is usually to export cc and CC to where your compiler is located, but you may need to explicitly edit the Makefiles to use icc/icpc for example.}

\item{If there is no HDF5 you will have to build it yourself:}
\begin{verbatim}
Obtain the source at https://support.hdfgroup.org/HDF5/release/obtainsrc.html
Extract the file, change to the directory and configure the build.
For building parallel HDF5:
./configure --enable-parallel cc=/path/to/mpicc CC=/path/to/mpicc
make
make install
\end{verbatim} 

\item{Depending on the machine you may need to add your HDF5/CUDA library locations to your \verb|LD_LIBRARY_PATH|.}

\item{Job submission scripts will often need module load commands in them, and \verb|LD_LIBRARY_PATH| needs to be exported with the HDF5/CUDA library locations.}

\end{enumerate}

\end{document}